{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'cl100k_base'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check encoding model for our llm\n",
    "llm = \"gpt-3.5-turbo\"\n",
    "tiktoken.encoding_for_model(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'cl100k_base'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tokenizer based on our llm's encoding model\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_name = \"cl100k_base\"\n",
    "text_splitter = TokenTextSplitter.from_tiktoken_encoder(\n",
    "    # encoding_name=encoder_name,\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    allowed_special={'<|endoftext|>'}\n",
    "    # separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>comment</th>\n",
       "      <th>doi</th>\n",
       "      <th>entry_id</th>\n",
       "      <th>journal_ref</th>\n",
       "      <th>pdf_url</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>published</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>updated</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'name': 'Zhuang Liu'}, {'name': 'Hanzi Mao'}...</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>CVPR 2022; Code: https://github.com/facebookre...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://arxiv.org/abs/2201.03545v2</td>\n",
       "      <td>None</td>\n",
       "      <td>http://arxiv.org/pdf/2201.03545</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>1641841150000</td>\n",
       "      <td>The \"Roaring 20s\" of visual recognition began ...</td>\n",
       "      <td>A ConvNet for the 2020s</td>\n",
       "      <td>1646233696000</td>\n",
       "      <td>A ConvNet for the 2020s\\n\\nZhuang Liu1,2* Hanz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'name': 'Ze Liu'}, {'name': 'Yutong Lin'}, {...</td>\n",
       "      <td>[cs.CV, cs.LG]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://arxiv.org/abs/2103.14030v2</td>\n",
       "      <td>None</td>\n",
       "      <td>http://arxiv.org/pdf/2103.14030</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>1616695171000</td>\n",
       "      <td>This paper presents a new vision Transformer, ...</td>\n",
       "      <td>Swin Transformer: Hierarchical Vision Transfor...</td>\n",
       "      <td>1629218494000</td>\n",
       "      <td>Swin Transformer: Hierarchical Vision Transfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'name': 'Alexey Dosovitskiy'}, {'name': 'Luc...</td>\n",
       "      <td>[cs.CV, cs.AI, cs.LG]</td>\n",
       "      <td>Fine-tuning code and pre-trained models are av...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://arxiv.org/abs/2010.11929v2</td>\n",
       "      <td>None</td>\n",
       "      <td>http://arxiv.org/pdf/2010.11929</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>1603389359000</td>\n",
       "      <td>While the Transformer architecture has become ...</td>\n",
       "      <td>An Image is Worth 16x16 Words: Transformers fo...</td>\n",
       "      <td>1622725736000</td>\n",
       "      <td>1\\n2\\n0\\n2\\n\\nn\\nu\\nJ\\n\\n3\\n\\n]\\n\\nV\\nC\\n.\\ns\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'name': 'Dan Friedman'}, {'name': 'Alexander...</td>\n",
       "      <td>[cs.LG, cs.CL]</td>\n",
       "      <td>Our code, and example Transformer Programs, ar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://arxiv.org/abs/2306.01128v1</td>\n",
       "      <td>None</td>\n",
       "      <td>http://arxiv.org/pdf/2306.01128</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>1685651221000</td>\n",
       "      <td>Recent research in mechanistic interpretabilit...</td>\n",
       "      <td>Learning Transformer Programs</td>\n",
       "      <td>1685651221000</td>\n",
       "      <td>3\\n2\\n0\\n2\\n\\nn\\nu\\nJ\\n\\n1\\n\\n]\\n\\nG\\nL\\n.\\ns\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'name': 'Lingjiao Chen'}, {'name': 'Matei Za...</td>\n",
       "      <td>[cs.LG, cs.AI, cs.CL, cs.SE]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://arxiv.org/abs/2305.05176v1</td>\n",
       "      <td>None</td>\n",
       "      <td>http://arxiv.org/pdf/2305.05176</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>1683609062000</td>\n",
       "      <td>There is a rapidly growing number of large lan...</td>\n",
       "      <td>FrugalGPT: How to Use Large Language Models Wh...</td>\n",
       "      <td>1683609062000</td>\n",
       "      <td>3\\n2\\n0\\n2\\n\\ny\\na\\nM\\n9\\n\\n]\\n\\nG\\nL\\n.\\ns\\nc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             authors  \\\n",
       "0  [{'name': 'Zhuang Liu'}, {'name': 'Hanzi Mao'}...   \n",
       "1  [{'name': 'Ze Liu'}, {'name': 'Yutong Lin'}, {...   \n",
       "2  [{'name': 'Alexey Dosovitskiy'}, {'name': 'Luc...   \n",
       "3  [{'name': 'Dan Friedman'}, {'name': 'Alexander...   \n",
       "4  [{'name': 'Lingjiao Chen'}, {'name': 'Matei Za...   \n",
       "\n",
       "                     categories  \\\n",
       "0                       [cs.CV]   \n",
       "1                [cs.CV, cs.LG]   \n",
       "2         [cs.CV, cs.AI, cs.LG]   \n",
       "3                [cs.LG, cs.CL]   \n",
       "4  [cs.LG, cs.AI, cs.CL, cs.SE]   \n",
       "\n",
       "                                             comment  doi  \\\n",
       "0  CVPR 2022; Code: https://github.com/facebookre...  NaN   \n",
       "1                                               None  NaN   \n",
       "2  Fine-tuning code and pre-trained models are av...  NaN   \n",
       "3  Our code, and example Transformer Programs, ar...  NaN   \n",
       "4                                               None  NaN   \n",
       "\n",
       "                            entry_id journal_ref  \\\n",
       "0  http://arxiv.org/abs/2201.03545v2        None   \n",
       "1  http://arxiv.org/abs/2103.14030v2        None   \n",
       "2  http://arxiv.org/abs/2010.11929v2        None   \n",
       "3  http://arxiv.org/abs/2306.01128v1        None   \n",
       "4  http://arxiv.org/abs/2305.05176v1        None   \n",
       "\n",
       "                           pdf_url primary_category      published  \\\n",
       "0  http://arxiv.org/pdf/2201.03545            cs.CV  1641841150000   \n",
       "1  http://arxiv.org/pdf/2103.14030            cs.CV  1616695171000   \n",
       "2  http://arxiv.org/pdf/2010.11929            cs.CV  1603389359000   \n",
       "3  http://arxiv.org/pdf/2306.01128            cs.LG  1685651221000   \n",
       "4  http://arxiv.org/pdf/2305.05176            cs.LG  1683609062000   \n",
       "\n",
       "                                             summary  \\\n",
       "0  The \"Roaring 20s\" of visual recognition began ...   \n",
       "1  This paper presents a new vision Transformer, ...   \n",
       "2  While the Transformer architecture has become ...   \n",
       "3  Recent research in mechanistic interpretabilit...   \n",
       "4  There is a rapidly growing number of large lan...   \n",
       "\n",
       "                                               title        updated  \\\n",
       "0                            A ConvNet for the 2020s  1646233696000   \n",
       "1  Swin Transformer: Hierarchical Vision Transfor...  1629218494000   \n",
       "2  An Image is Worth 16x16 Words: Transformers fo...  1622725736000   \n",
       "3                      Learning Transformer Programs  1685651221000   \n",
       "4  FrugalGPT: How to Use Large Language Models Wh...  1683609062000   \n",
       "\n",
       "                                                text  \n",
       "0  A ConvNet for the 2020s\\n\\nZhuang Liu1,2* Hanz...  \n",
       "1  Swin Transformer: Hierarchical Vision Transfor...  \n",
       "2  1\\n2\\n0\\n2\\n\\nn\\nu\\nJ\\n\\n3\\n\\n]\\n\\nV\\nC\\n.\\ns\\...  \n",
       "3  3\\n2\\n0\\n2\\n\\nn\\nu\\nJ\\n\\n1\\n\\n]\\n\\nG\\nL\\n.\\ns\\...  \n",
       "4  3\\n2\\n0\\n2\\n\\ny\\na\\nM\\n9\\n\\n]\\n\\nG\\nL\\n.\\ns\\nc...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json(\"arxiv_data_add_clean.json\")\n",
    "print(len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
    "from uuid import uuid4\n",
    "import time\n",
    "model_name = 'text-embedding-ada-002'\n",
    "\n",
    "# embed = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-base\")\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to split the 'text' column into chunks for each row\n",
    "\n",
    "batch_limit = 3\n",
    "texts = []\n",
    "metadatas = []\n",
    "for i, text in enumerate(data['text']):\n",
    "    metadata = {\n",
    "        # \"authors\": [author['name'] for author in data.iloc[i]['authors']],\n",
    "        \"pdf_url\": data.iloc[i]['pdf_url'],\n",
    "        # \"summary\": data.iloc[i]['summary'],\n",
    "        \"title\": data.iloc[i]['title']\n",
    "    }\n",
    "    chunks = text_splitter.split_text(\n",
    "        text\n",
    "    )\n",
    "\n",
    "    chunks_metadata = [{\n",
    "        \"chunk_id\": j, \"chunk\": text, **metadata\n",
    "    } for j, text in enumerate(chunks)]\n",
    "\n",
    "    texts.extend(chunks)\n",
    "    metadatas.extend(chunks_metadata)\n",
    "    # if i == 10:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>chunk</th>\n",
       "      <th>pdf_url</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A ConvNet for the 2020s\\n\\nZhuang Liu1,2* Hanz...</td>\n",
       "      <td>http://arxiv.org/pdf/2201.03545</td>\n",
       "      <td>A ConvNet for the 2020s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\\nexploration is a family of pure ConvNet mode...</td>\n",
       "      <td>http://arxiv.org/pdf/2201.03545</td>\n",
       "      <td>A ConvNet for the 2020s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[59], ushering in a new\\nera of computer visi...</td>\n",
       "      <td>http://arxiv.org/pdf/2201.03545</td>\n",
       "      <td>A ConvNet for the 2020s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>, as the Transformers replaced recurrent neura...</td>\n",
       "      <td>http://arxiv.org/pdf/2201.03545</td>\n",
       "      <td>A ConvNet for the 2020s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>work in this\\ndirection, demonstrating for th...</td>\n",
       "      <td>http://arxiv.org/pdf/2201.03545</td>\n",
       "      <td>A ConvNet for the 2020s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk_id                                              chunk  \\\n",
       "0         0  A ConvNet for the 2020s\\n\\nZhuang Liu1,2* Hanz...   \n",
       "1         1  \\nexploration is a family of pure ConvNet mode...   \n",
       "2         2   [59], ushering in a new\\nera of computer visi...   \n",
       "3         3  , as the Transformers replaced recurrent neura...   \n",
       "4         4   work in this\\ndirection, demonstrating for th...   \n",
       "\n",
       "                           pdf_url                    title  \n",
       "0  http://arxiv.org/pdf/2201.03545  A ConvNet for the 2020s  \n",
       "1  http://arxiv.org/pdf/2201.03545  A ConvNet for the 2020s  \n",
       "2  http://arxiv.org/pdf/2201.03545  A ConvNet for the 2020s  \n",
       "3  http://arxiv.org/pdf/2201.03545  A ConvNet for the 2020s  \n",
       "4  http://arxiv.org/pdf/2201.03545  A ConvNet for the 2020s  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_chunks = pd.DataFrame(metadatas)\n",
    "data_chunks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The substring <|endoftext|> occurs 1864 times before replacement.\n",
      "The substring <|endoftext|> occurs 1864 times after replacement.\n"
     ]
    }
   ],
   "source": [
    "# Convert 'chunk' column to lower-case and strip leading/trailing whitespace\n",
    "data_chunks['chunk'] = data_chunks['chunk'].str.lower().str.strip()\n",
    "\n",
    "# Count occurrences of '<hello>' before replacement\n",
    "count_before = data_chunks['chunk'].str.count('<|endoftext|>').sum()\n",
    "print(f'The substring <|endoftext|> occurs {count_before} times before replacement.')\n",
    "\n",
    "# Replace '<|endoftext|>' with empty string\n",
    "data_chunks['chunk'] = data_chunks['chunk'].str.replace('<|endoftext|>', '', regex=False)\n",
    "\n",
    "# Count occurrences of '<|endoftext|>' after replacement\n",
    "count_after = data_chunks['chunk'].str.count('<|endoftext|>').sum()\n",
    "print(f'The substring <|endoftext|> occurs {count_after} times after replacement.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_docs = DataFrameLoader(data_chunks, page_content_column='chunk')\n",
    "docs = data_docs.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4434"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_batch1 = docs[:10]\n",
    "docs_batch2 = docs[10:20]\n",
    "docs_batch3 = docs[2000:3000]\n",
    "docs_batch4 = docs[3000:4000]\n",
    "docs_batch5 = docs[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store\n",
    "vectorstore = FAISS.from_documents(docs_batch1, embed)\n",
    "\n",
    "# Add to vector store (work around from OpenAI rate limit error)\n",
    "vectorstore.aadd_documents(docs_batch2) # do this for all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what can you tell me about GPT?\"\n",
    "\n",
    "# Similarity search\n",
    "test_search = vectorstore.similarity_search_with_score(\n",
    "    query,  # our search query\n",
    "    k=3  # return 3 most relevant docs\n",
    ")\n",
    "\n",
    "# Extract text content and metadata from query results\n",
    "first_chunk = test_search[0][0].page_content\n",
    "first_metadata = test_search[0][0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.save_local(\"test_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vectorstore = FAISS.load_local(\"test_index\", embeddings=embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=', as the transformers replaced recurrent neural\\nnetworks to become the dominant backbone architecture.\\ndespite the disparity in the task of interest between language\\nand vision domains, the two streams surprisingly converged\\nin the year 2020, as the introduction of vision transformers\\n(vit) completely altered the landscape of network architec-\\nture design. except for the initial “patchify” layer, which\\nsplits an image into a sequence of patches, vit introduces no\\nimage-speciﬁc inductive bias and makes minimal changes\\nto the original nlp transformers. one primary focus of\\nvit is on the scaling behavior: with the help of larger model\\nand dataset sizes, transformers can outperform standard\\nresnets by a signiﬁcant margin. those results on image\\nclassiﬁcation tasks are inspiring, but computer vision is not\\nlimited to image classiﬁcation. as discussed previously,\\nsolutions to numerous computer vision tasks in the past\\ndecade depended signiﬁcantly on a sliding-window, fully-\\nconvolutional paradigm. without the convnet inductive\\nbiases, a vanilla vit model faces many challenges in being\\nadopted as a generic vision backbone. the biggest chal-\\nlenge is vit’s global attention design, which has a quadratic\\ncomplexity with respect to the input size. this might be\\nacceptable for imagenet classiﬁcation, but quickly becomes\\nintractable with higher-resolution inputs.\\n\\nhierarchical transformers employ a hybrid approach to\\nbridge this gap. for example, the “sliding window” strategy\\n(e.g. attention within local windows) was reintroduced to\\ntransformers, allowing them to behave more similarly to\\nconvnets. swin transformer [45] is a milestone work in this\\ndirection, demonstrating for the ﬁrst time that transformers\\ncan be adopted as a generic vision backbone and achieve\\nstate-of-the-art performance across a range of computer vi-\\nsion tasks beyond image classiﬁcation. swin transformer’s\\nsuccess and rapid adoption also revealed one thing:\\nthe\\nessence of convolution is not becoming irrelevant; rather, it\\nremains much desired and has never faded.', metadata={'chunk_id': 3, 'pdf_url': 'http://arxiv.org/pdf/2201.03545', 'title': 'A ConvNet for the 2020s'}),\n",
       " Document(page_content='exploration is a family of pure convnet models dubbed con-\\nvnext. constructed entirely from standard convnet modules,\\nconvnexts compete favorably with transformers in terms of\\naccuracy and scalability, achieving 87.8% imagenet top-1\\naccuracy and outperforming swin transformers on coco\\ndetection and ade20k segmentation, while maintaining the\\nsimplicity and efﬁciency of standard convnets.\\n\\n1. introduction\\n\\nlooking back at the 2010s, the decade was marked by\\nthe monumental progress and impact of deep learning. the\\nprimary driver was the renaissance of neural networks, partic-\\nularly convolutional neural networks (convnets). through\\nthe decade, the ﬁeld of visual recognition successfully\\nshifted from engineering features to designing (convnet)\\narchitectures. although the invention of back-propagation-\\ntrained convnets dates all the way back to the 1980s [42],\\nit was not until late 2012 that we saw its true potential for\\n\\n*work done during an internship at facebook ai research.\\n†corresponding author.\\n\\nfigure 1. imagenet-1k classiﬁcation results for • convnets and\\n◦ vision transformers. each bubble’s area is proportional to flops\\nof a variant in a model family. imagenet-1k/22k models here\\ntake 2242/3842 images respectively. resnet and vit results were\\nobtained with improved training procedures over the original papers.\\nwe demonstrate that a standard convnet model can achieve the\\nsame level of scalability as hierarchical vision transformers while\\nbeing much simpler in design.\\n\\nvisual feature learning. the introduction of alexnet [40]\\nprecipitated the “imagenet moment” [59], ushering in a new\\nera of computer vision. the ﬁeld has since evolved at a\\nrapid speed. representative convnets like vggnet [64],\\ninceptions [68], resne(x)t [28, 87], densenet [36], mo-\\nbilenet [34], efﬁcientnet [71] and regnet [54] focused on\\ndifferent aspects of accuracy, ef�', metadata={'chunk_id': 1, 'pdf_url': 'http://arxiv.org/pdf/2201.03545', 'title': 'A ConvNet for the 2020s'}),\n",
       " Document(page_content='[59], ushering in a new\\nera of computer vision. the ﬁeld has since evolved at a\\nrapid speed. representative convnets like vggnet [64],\\ninceptions [68], resne(x)t [28, 87], densenet [36], mo-\\nbilenet [34], efﬁcientnet [71] and regnet [54] focused on\\ndifferent aspects of accuracy, efﬁciency and scalability, and\\npopularized many useful design principles.\\n\\nthe full dominance of convnets in computer vision was\\nnot a coincidence: in many application scenarios, a “sliding\\nwindow” strategy is intrinsic to visual processing, particu-\\nlarly when working with high-resolution images. convnets\\nhave several built-in inductive biases that make them well-\\nsuited to a wide variety of computer vision applications. the\\nmost important one is translation equivariance, which is a de-\\nsirable property for tasks like objection detection. convnets\\nare also inherently efﬁcient due to the fact that when used in\\na sliding-window manner, the computations are shared [62].\\nfor many decades, this has been the default use of convnets,\\ngenerally on limited object categories such as digits [43],\\nfaces [58, 76] and pedestrians [19, 63]. entering the 2010s,\\n\\n4816256gflopsdiameter \\n \\n \\n \\n \\n \\n\\x0cthe region-based detectors [23, 24, 27, 57] further elevated\\nconvnets to the position of being the fundamental building\\nblock in a visual recognition system.\\n\\naround the same time, the odyssey of neural network\\ndesign for natural language processing (nlp) took a very\\ndifferent path, as the transformers replaced recurrent neural\\nnetworks to become the dominant backbone architecture.\\ndespite the disparity in the task of interest between language\\nand vision domains, the two streams surprisingly converged\\nin the year 2020, as the introduction of vision transformers\\n(vit) completely altered the landscape of network architec-\\nture design. except for the initial “patchify” layer, which\\nsplits an image into a sequence of patches, vit introduces no', metadata={'chunk_id': 2, 'pdf_url': 'http://arxiv.org/pdf/2201.03545', 'title': 'A ConvNet for the 2020s'})]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what can you tell me about BERT?\"\n",
    "\n",
    "loaded_vectorstore.similarity_search(\n",
    "    query,  # our search query\n",
    "    k=3  # return 3 most relevant docs\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
